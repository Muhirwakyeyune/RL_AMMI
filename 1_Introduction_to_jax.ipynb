{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a_vbrBxWc8O2",
        "sBchwwuF7ukt",
        "rX0_U5lHZuCV",
        "2nTAO_zbGXvp",
        "gSRhzqFu8BRy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vbrBxWc8O2"
      },
      "source": [
        "# Licence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTl-ivwIcS38"
      },
      "source": [
        "#Licence\n",
        "#Copyright 2021 Google LLC.\n",
        "#SPDX-License-Identifier: Apache-2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErkTGyQ0Qase"
      },
      "source": [
        "#JAX Tutorial\n",
        "This first practical aims at getting familliar with some of the tools we will be using in the next practicals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVaNs4WEQekY"
      },
      "source": [
        "##Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDZSbrcjQkJ5"
      },
      "source": [
        "#@title Installations  { form-width: \"30%\" }\n",
        "\n",
        "%pip install git+https://github.com/deepmind/acme.git#egg=dm-acme[jax,tf,envs]\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBqd3jWPQ6YJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "71f034ca-0662-4ef4-9f4c-6c06d70e52ea",
        "cellView": "form"
      },
      "source": [
        "#@title Imports  { form-width: \"30%\" }\n",
        "\n",
        "from typing import *\n",
        "import IPython\n",
        "\n",
        "import base64\n",
        "import chex\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import dm_env\n",
        "import enum\n",
        "import functools\n",
        "import gym\n",
        "import haiku as hk\n",
        "import io\n",
        "import itertools\n",
        "import jax\n",
        "from jax import tree_util\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import reverb\n",
        "import rlax\n",
        "import time\n",
        "from bsuite import environments\n",
        "import bsuite.environments.catch as dm_catch\n",
        "\n",
        "import warnings\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOeElBpmsYFo"
      },
      "source": [
        "# Introduction to JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03tnkC41spVL"
      },
      "source": [
        "To implement our RL algorithms, we will resort to neural networks as our function approximators, and we will train them using gradient based optimizers. To make that easy, we will use <a href=\"https://github.com/google/jax\">JAX</a>, to get access to easy gradient computations with a numpy-like flavor, and <a href=\"https://github.com/deepmind/dm-haiku\">Haiku</a>, to easily define our neural network architectures. If you are familiar with other frameworks, JAX/Haiku respectively corresponds to tensorflow/keras pytorch/pytorch.nn.\n",
        "\n",
        "If you need further tutorial, go to: https://jax.readthedocs.io/en/latest/jax-101/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. JAX Basics"
      ],
      "metadata": {
        "id": "sBchwwuF7ukt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXqsxL35udKp"
      },
      "source": [
        "JAX is a numerical computation library, very close to numpy for its basic use, that allows one to easily execute operations on GPU, and gives access to <a href=\"https://github.com/google/jax#transformations\">numerical function transformations</a>, that allows for gradient computations, automatic vectorization, or jitting.\n",
        "\n",
        "JAX has a _functional_ flavor; to be able to use numerical function transformations, you will have to define _pure_ functions, i.e. mathematical functions, whose result do not depend on the context in which they are used.\n",
        "\n",
        "For instance the following function is pure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7avvHR67yA11"
      },
      "source": [
        "def pure_function(x: chex.Array) -> chex.Array:\n",
        "  return 3 * x + jnp.tanh(2 * x) / (x ** 2 + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI-lybNOy481"
      },
      "source": [
        "The following method is not pure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D3KcpERzJqJ"
      },
      "source": [
        "class Counter:\n",
        "  def __init__(self) -> None:\n",
        "    self._i = 0.\n",
        "\n",
        "  def unpure_function(self, x: chex.Array) -> chex.Array:\n",
        "    self._i = self._i + 1.\n",
        "    return self._i * x + jnp.tanh(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrLn-PEAz57d"
      },
      "source": [
        "Given a pure function, you can easily obtain the associated gradient function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU0Jh8Za0DL_"
      },
      "source": [
        "grad_pure = jax.grad(pure_function)\n",
        "x = 3.\n",
        "print(f'Value at point x={x}, f(x)={pure_function(x)}, grad_f(x)={grad_pure(x)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeUZPP0nF53M"
      },
      "source": [
        "In addition to `jax.grad`, JAX provides `jax.vmap` for automatic vectorization, `jax.jit` for jitting (to fully make use of specialized hardware) and `jax.pmap`, to automatically distribute functions accross devices.\n",
        "\n",
        "For instance, if you want to have a batched version of matrix multiplication, you can use the usual matrix multiplication, and directly vmap it in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3KFxDh3Tt2m"
      },
      "source": [
        "batch_matrix_multiply = jax.vmap(lambda a, b: a @ b)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng_a, rng_b = jax.random.split(rng)\n",
        "a = jax.random.normal(key=rng_a, shape=(12, 5, 7))\n",
        "b = jax.random.normal(key=rng_b, shape=(12, 7, 9))\n",
        "print(batch_matrix_multiply(a, b).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcpax51lUnhP"
      },
      "source": [
        "In this example, we have been hitting one of the differences between JAX numpy and numpy. Numpy handles random seeds _implicitly_, when you want a random\n",
        "number, you get one by simply calling one of numpy's functions, and the number\n",
        "will depend on numpy global seed. With JAX, random seeds are handled _explicitly_, and each function that needs to generate random numbers takes a random key as additional input. This has to do with the functional paradigm of JAX: if we were not handling the random key explicitly, each call to a random function would lead to a different result, breaking the pure function hypothesis. By passing the random key explicitly, we make sure that the same random function, called with the same random key, will produce the same result. As a side effect, this also make results produced using JAX easily reproducible, as it is easy to trace which random seeds have been used where. To\n",
        "know more about how JAX handles randomness, you can read <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#rngs-and-state\">this page<a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX0_U5lHZuCV"
      },
      "source": [
        "### ***Exercises***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnMpTZq_Zwy5"
      },
      "source": [
        "As a first exercise, we are giving you the following function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9903KIgZs3J"
      },
      "source": [
        "def func(x: chex.Array) -> chex.Array:\n",
        "  return x ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugrEP9JIZ630"
      },
      "source": [
        "which simply computes the square of an array. By using simple jax transformations can you get a function that takes a batch of scalars, and outputs the value of the gradient of the squared function for each element of the batch?\n",
        "\n",
        "**Hint:** jax.grad can only take as input a function that outputs a single scalar, so calling jax.grad directly on func and applying it to a vector won't work.\n",
        "\n",
        "Can you make this function run faster?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nj1m2vxahuG",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Batched gradients { form-width: \"30%\" }\n",
        "batched_grad = None\n",
        "fast_batched_grad = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2-_WBtvajb1",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Batched gradients { form-width: \"30%\" }\n",
        "solution_batched_grad = jax.vmap(jax.grad(func))\n",
        "jitted_solution = jax.jit(solution_batched_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkp0HdK3b0sT"
      },
      "source": [
        "You can test your solution by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV8aarewbuNp",
        "cellView": "form"
      },
      "source": [
        "#@title **[Test]** Batched gradients (Uncomment to run){ form-width: \"30%\" }\n",
        "key = jax.random.PRNGKey(0)\n",
        "normal = jax.random.normal(key=key, shape=(3,))\n",
        "if (fast_batched_grad(normal) == 2 * normal).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Provably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kiq645RbePv"
      },
      "source": [
        "Can you do the same for a batch of batches, without flattening your input? (i.e. you have a matrix of numbers, and you want a matrix containing the gradient for each of the numbers in the matrix.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcMNaIlBc1RD",
        "cellView": "form"
      },
      "source": [
        "#@title **[Implement]** Matrix gradients { form-width: \"30%\" }\n",
        "fast_matrix_grad = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxYugrUvdSMn",
        "cellView": "form"
      },
      "source": [
        "#@title **[Solution]** Matrix gradients { form-width: \"30%\" }\n",
        "jitted_solution_matrix = jax.jit(jax.vmap(jitted_solution))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq5_gmUkdZ1N"
      },
      "source": [
        "#@title **[Test]** Matrix gradients (Uncomment to run){ form-width: \"30%\" }\n",
        "key = jax.random.PRNGKey(0)\n",
        "normal = jax.random.normal(key=key, shape=(3, 3,))\n",
        "if (fast_matrix_grad(normal) == 2 * normal).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Probably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0v4pUxXduJV"
      },
      "source": [
        "Another very useful application of `vmap` is batched indexing. Assume you have a `[B1, B2, ..., BN]` tensor of indices `idx`, and a `[B1, B2, ..., BN, F]` tensor of features `features`, and for each element `i1, ..., iN`, you\n",
        "would like to  retrieve element `features[i1, ..., iN, idx[i1, ..., iN]]` from\n",
        "the feature tensor, can you do this easily using vmap? (maybe start with a fixed `N`, then generalize to all `N`'s.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBlJnA9hAGQ"
      },
      "source": [
        "#@title **[Implement]** Batched indexing { form-width: \"30%\" }\n",
        "def batched_indexing(idxs: chex.Array, features: chex.Array) -> chex.Array:\n",
        "  ##### IMPLEMENT #####\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBjZTSw7e86s"
      },
      "source": [
        "#@title **[Solution]** Batched indexing { form-width: \"30%\" }\n",
        "@jax.jit\n",
        "def solution_batched_indexing(idxs: chex.Array, features: chex.Array) -> chex.Array:\n",
        "  def simple_index(idx, feature):\n",
        "    return feature[idx]\n",
        "  batched_index = simple_index\n",
        "  for _ in range(idxs.ndim):\n",
        "    batched_index = jax.vmap(batched_index)\n",
        "  return batched_index(idxs, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdb3DRDUgXKa"
      },
      "source": [
        "#@title **[Test]** Batched indexing (Uncomment to run){ form-width: \"30%\" }\n",
        "inputs = jnp.array([[-0.196,  0.255,  0.573,  0.441, -0.847,  0.318,  0.646],\n",
        " [ 0.034, -0.889, -0.266, -1.561, -0.638, -0.442,  0.91 ],\n",
        " [-0.017,  0.758,  1.089,  0.299,  1.491,  0.079, -1.222],\n",
        " [ 0.952,  0.21,   1.386, -0.338,  2.952, -0.995, -0.516],\n",
        " [ 0.292, -0.143,  1.614,  1.643,  0.114,  0.254, -1.306],])\n",
        "outputs = jnp.array([ 0.255, -1.561,  0.079,  1.386, -1.306])\n",
        "idxs = jnp.array([1, 3, 5, 2, 6], dtype=jnp.int32)\n",
        "if (batched_indexing(idxs, inputs) == outputs).all():\n",
        "  print('Probably correct.')\n",
        "else:\n",
        "  print('Probably incorrect.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Let's talk about gradients"
      ],
      "metadata": {
        "id": "2nTAO_zbGXvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients manipulation are the core of deep learning and therefore in this class, you will have to use them quite a lot.\n",
        "\n",
        "In the previous section, you used the function [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) which allows you to evaluate the gradient of a function. The default behavior of `jax.grad` is to diffferentiate only with respect to the first argument. For example, have a look at the following:"
      ],
      "metadata": {
        "id": "7SvvfGudHna2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  return (x + y*y).sum()\n",
        "\n",
        "grad_my_func = jax.grad(my_func)\n",
        "\n",
        "# The gradient of this function with respect to x is a vector with the same\n",
        "# shape as x but filled with ones.\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "print(grad_my_func(test_x, test_y))\n",
        "print(grad_my_func(test_x, 2*test_y))"
      ],
      "metadata": {
        "id": "MUMYdHNvHmzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However in deep learning, we rarely want to compute the gradient of a single vector. Fortunately, jax gives us several ways to do that.\n",
        "\n",
        "For example, by using argument `argnums` we can tell jax to compute the gradient with respect to the argument at the given position. Let try it with our function:"
      ],
      "metadata": {
        "id": "cejZmZX9Jqug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  return (x + y*y).sum()\n",
        "\n",
        "# Now we are computing the gradient with respect to the second argument y\n",
        "grad_my_func = jax.grad(my_func, argnums=1)\n",
        "\n",
        "# This gradient is equal to 2*y\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "print(grad_my_func(test_x, test_y))\n",
        "print(grad_my_func(test_x, 2*test_y))"
      ],
      "metadata": {
        "id": "47klKpOUJqAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can provide argnums with a list of integer instead of one value, in this case, the function will return a gradient for each index you gave:"
      ],
      "metadata": {
        "id": "Jv3ZcaITMw_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  return (x + y*y).sum()\n",
        "\n",
        "# Now we are computing the gradient with respect to both arguments\n",
        "grad_my_func = jax.grad(my_func, argnums=(0, 1))\n",
        "\n",
        "# This gradient is equal to (1, 2*y)\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "\n",
        "# Notice that the function now outputs two values\n",
        "print(grad_my_func(test_x, test_y))\n",
        "print(grad_my_func(test_x, 2*test_y))"
      ],
      "metadata": {
        "id": "tc29p2qOM7yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise***\n",
        "\n",
        "Let's consider a function $f$, which given a set of parameters $(a, b, \\theta) \\in \\mathbb{R}^{NxNxM}$ computes the value:\n",
        "\n",
        "$ f(a, b, \\theta) = \\sum_{i=1}^N a_ib_i + \\sum_{j=1}^M \\theta_j $\n",
        "\n",
        "Code a function in jax which outputs the couple $(\\nabla f_a, \\nabla f_\\theta)$."
      ],
      "metadata": {
        "id": "UuEqcnt5K19M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here !\n",
        "\n",
        "## Test it with the following values\n",
        "test_a = jnp.asarray([1., 1.])\n",
        "test_b = jnp.asarray([2., 2.])\n",
        "test_theta = jnp.asarray([2., 2., 2.])"
      ],
      "metadata": {
        "id": "yvy1Zhx1M8Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better, but in deep learning we usually don't handle just a pair of vectors: we deal with hundreds of them (with their gradient) at the same time. So how are we going to pass all theses gradients around ? Well, it happens that jax also works with dictionary of parameters, or any nested structure containing dicts, tuples or lists."
      ],
      "metadata": {
        "id": "FmJfxl81NQkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_fun_with_lots_of_params(params : Mapping[str, chex.Array]) -> float:\n",
        "  x = params['x']\n",
        "  y = params['y']\n",
        "  z = params['z']\n",
        "  return  (x**2 + 2*y + x*z).sum()\n",
        "\n",
        "# Now let's compute the gradient\n",
        "grad_my_func = jax.grad(my_fun_with_lots_of_params)\n",
        "\n",
        "test_x = jnp.asarray([1., 1.])\n",
        "test_y = jnp.asarray([2., 2.])\n",
        "test_z = jnp.asarray([3., 3.])\n",
        "\n",
        "params_dict = {'x' : test_x, 'y': test_y, 'z' : test_z}\n",
        "\n",
        "# Now my gradient outputs a dictionary of values\n",
        "print(grad_my_func(params_dict))"
      ],
      "metadata": {
        "id": "IXIJoGgqNv2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise***\n",
        "\n",
        "Compute the gradients $(\\nabla f_a, \\nabla f_\\theta)$ where $f$ is the function defined in the previous exercise but this time use a dictionary containing the parameters $(a, \\theta)$."
      ],
      "metadata": {
        "id": "SoU7BOzwO6eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code here\n",
        "\n",
        "## Test it with the following values\n",
        "test_a = jnp.asarray([1., 1.])\n",
        "test_b = jnp.asarray([2., 2.])\n",
        "test_theta = jnp.asarray([2., 2., 2.])"
      ],
      "metadata": {
        "id": "8bIUs477O4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Just in time compilation (jit)\n"
      ],
      "metadata": {
        "id": "gSRhzqFu8BRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jax allows you to compile some portions of your python code as you run it. This process, called just in time compilation or [jitting](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) can substantially accelerate your code if used properly."
      ],
      "metadata": {
        "id": "d4DWGiND8RPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of jitting\n",
        "import time\n",
        "\n",
        "def add_and_square(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  out = x**2\n",
        "  out = out+ y * 12\n",
        "  out = out**3\n",
        "  return out\n",
        "\n",
        "jitted_add = jax.jit(add_and_square)\n",
        "\n",
        "n_runs = 1000\n",
        "x = jnp.zeros((3,14), dtype=float)\n",
        "y = 2 + jnp.zeros((3,14), dtype=float)\n",
        "t_start = time.time()\n",
        "for _ in range(n_runs):\n",
        "  add_and_square(x, y)\n",
        "duration = time.time() - t_start\n",
        "print(f\"Without jitting, ran {n_runs} in {duration} seconds.\")\n",
        "\n",
        "t_start = time.time()\n",
        "for _ in range(n_runs):\n",
        "  jitted_add(x, y)\n",
        "duration = time.time() - t_start\n",
        "print(f\"With jitting, ran {n_runs} in {duration} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQXLdzhJ8xtA",
        "outputId": "8c97d470-6307-4a2e-fa3f-ca972071bb74"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without jitting, ran 1000 in 1.1659579277038574 seconds.\n",
            "With jitting, ran 1000 in 0.08092808723449707 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of jitting itself takes time, for this reason you should follow this good practicies when using `jax.jit`:\n",
        "- Never jit inside a loop, jit your function ahead of time or use the `@jax.jit` decorator.\n",
        "- Try to jit chunks of code that are as big and complex as possible"
      ],
      "metadata": {
        "id": "Y9GkbyRFIzXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not do\n",
        "def add(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  out = x + y\n",
        "  return out\n",
        "\n",
        "def square(x : chex.Array) -> chex.Array:\n",
        "  out = x**2\n",
        "  return out\n",
        "\n",
        "def log(x : chex.Array) -> chex.Array:\n",
        "  out = jnp.log(x)\n",
        "  return out\n",
        "\n",
        "n_runs = 100\n",
        "x = jnp.zeros((3,14), dtype=float)\n",
        "y = 2 + jnp.zeros((3,14), dtype=float)\n",
        "t_start = time.time()\n",
        "for _ in range(n_runs):\n",
        "  jit_add = jax.jit(add)\n",
        "  jit_square = jax.jit(square)\n",
        "  jit_log = jax.jit(log)\n",
        "  jit_log(jit_square(jit_add(x, y)))\n",
        "duration = time.time() - t_start\n",
        "print(f\"Poorly used jitting: {n_runs} runs in {duration} seconds.\")\n",
        "\n",
        "# But do\n",
        "def add_square_log(x: chex.Array, y: chex.Array) -> chex.Array:\n",
        "  out = x + y\n",
        "  out = out**2\n",
        "  return jnp.log(out)\n",
        "\n",
        "jit_add_square_log = jax.jit(add_square_log)\n",
        "t_start = time.time()\n",
        "for _ in range(n_runs):\n",
        "  jit_add_square_log(x,y)\n",
        "duration = time.time() - t_start\n",
        "print(f\"Proper jitting: {n_runs} runs in {duration} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj-yUmaoJKrZ",
        "outputId": "f568f4a5-df50-41be-f583-f4ed0b39b1cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poorly used jitting: 100 runs in 0.3196744918823242 seconds.\n",
            "Proper jitting: 100 runs in 0.05981945991516113 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you need to remember that `jax.jit` compile a function by **executing it**. This means that branches of code hidden behind conditions like a `if` for example won't be explored and therefore won't be compiled by `jax.jit`. For example have a look at the following code:"
      ],
      "metadata": {
        "id": "7f97pyOXLGYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It doesn't work !\n",
        "def my_func(x: chex.Array):\n",
        "  if jnp.sum(x) > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "pan = jax.jit(my_func)\n",
        "pan(jnp.zeros((12,3)))"
      ],
      "metadata": {
        "id": "JWZ_yXF9LiJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what if you actually need conditions or branches ? Well in this case you should tell `jax.jit` that some of input arguments of your function are **statics**. In practice, this means that jax will jit your function for the first value of the static argument it encounter, but will recompile your function again each time this value changes."
      ],
      "metadata": {
        "id": "zBqo4EuMLzIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_forward(x : chex.Array, is_training : bool) -> chex.Array:\n",
        "  if is_training:\n",
        "    return x**2\n",
        "  else:\n",
        "    return -x\n",
        "\n",
        "f_jit_correct = jax.jit(dummy_forward, static_argnums=1)\n",
        "\n",
        "# The function is compiled a first time here\n",
        "x = jnp.zeros((12, 3))\n",
        "f_jit_correct(x, True)\n",
        "\n",
        "# No further compilation here\n",
        "f_jit_correct(x + 1, True)\n",
        "\n",
        "# The function is compiled again here\n",
        "f_jit_correct(x +1, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUadjqx8NK2P",
        "outputId": "2db8c8c3-a630-481d-9d26-28b6b558b14f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.],\n",
              "             [-1., -1., -1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this reason, **you shouldn't use static arguments if you know they will change often in your code**. It would make your execution painfully slow as jax would spend its time compiling stuff rather than executing your code."
      ],
      "metadata": {
        "id": "f7AFvPYeMboH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. A common JAX caveat"
      ],
      "metadata": {
        "id": "OXqaYSLYsi_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is one mistake that is recurrent in newcomer's JAX code, and that you will probably make at some point, which consists in forgetting that JAX function should remain pure, and will badly handle (impure) side effects. We present here a simple case where this mistake is made and the corresponding code does not behave as one could imagine."
      ],
      "metadata": {
        "id": "yWTo1GZjssEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterAndJax:\n",
        "  def __init__(self) -> None:\n",
        "    self._counter = 0\n",
        "\n",
        "  def increment(self) -> None:\n",
        "    self._counter += 1\n",
        "\n",
        "  def apply(self, x: chex.Array) -> chex.Array:\n",
        "    return x + self._counter"
      ],
      "metadata": {
        "id": "YAGOPC7utNdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class `CounterAndJax` maintains a counter and has an apply method that can be applied to a `chex.Array`. We already see that `apply` is not a pure function. `apply` internally uses the `CounterAndJax` attribute `_counter`, which is not provided as an explicit argument. Can you guess what will be the result of the following computation:"
      ],
      "metadata": {
        "id": "mhA3WhJXtv72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "caj.apply(jnp.zeros((3,)))"
      ],
      "metadata": {
        "id": "o2Hkj-0iudFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It did the _correct_ thing, of actually incrementing the counter, then using the new value when applied. So why are we even bothering with only using pure functions? Let's try something else. Can you guess what the following code will print out?"
      ],
      "metadata": {
        "id": "6vBzuQPpvRoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "apply = jax.jit(caj.apply)\n",
        "apply(jnp.zeros((3,)))\n",
        "caj.increment()\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))"
      ],
      "metadata": {
        "id": "HY1U6OIDvhqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The apply function returned ones instead of threes, the two last increments were not taken into account. This is because the `caj.apply` function was jitted, and thus anything within this function except from its arguments was considered as static at compile time, and frozen when producing the `jaxpr`. After the jitting pass, `caj.increment` does not affect `apply` `jaxpr`, and thus the result of the computations. Now for some even trickier behavior, can you guess what the following cell prints out?"
      ],
      "metadata": {
        "id": "CknB82AjwUro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "apply = jax.jit(caj.apply)\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))"
      ],
      "metadata": {
        "id": "yJkNK4f3w7K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a strange one. What you must remember to answer this question properly is that JAX does not jit your function when `jax.jit` is called, but when the resulting function is applied, **because it needs to know the shape of the arguments you are passing in**. In that case, `apply` is jitted when it is called on the first `jnp.zeros((3,))` tensor, after two increments have been done. Let's finish with the trickiest of all, can you guess what the following code will produce?"
      ],
      "metadata": {
        "id": "xI8buolOxANQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caj = CounterAndJax()\n",
        "caj.increment()\n",
        "apply = jax.jit(caj.apply)\n",
        "caj.increment()\n",
        "apply(jnp.zeros((3,)))\n",
        "caj.increment()\n",
        "apply(jnp.zeros((4,)))\n",
        "caj.increment()\n",
        "apply(jnp.zeros((4,)))"
      ],
      "metadata": {
        "id": "yybLpmSFxwCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, you must predict where the apply function will be jitted for the specific argument shape you are using. In this case, the first call `apply(jnp.zeros((4,)))` is where the function is first jitted for 1D tensors of size 4. This comes after three counter increments, and the last counter increment has not effect.\n",
        "\n",
        "As you may notice, predicting JAX's behavior when side effects are involved is extremely complicated (close to impossible in very complex cases). This is the reason why you should try to only use pure functions when you are using JAX."
      ],
      "metadata": {
        "id": "0EwHPIKtx1WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. A simple linear regression in JAX"
      ],
      "metadata": {
        "id": "UxFzCbg9tHAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test our newly acquired skills, we are going to implement a simple linear regression algorithm on a fixed dataset. You are going to be provided with an input output dataset, with inputs $X \\in \\mathbb{R}^{d \\times 6}$ and outputs $Y \\in \\mathbb{R}^{d \\times 12}$, where $d$ is the dataset size.\n",
        "\n",
        "We are going to optimize two sets of parameters, $W \\in \\mathbb{R}^{6 \\times 12}$, some weights, and $b \\in \\mathbb{R}^{12}$, some biases, to minimize the mean squared error\n",
        "$$\\mathcal{L}_{W, b}(\\mathbf{y}) = \\frac{1}{d}\\sum\\limits_i \\|x_i W + b - y_i\\|^2$$\n",
        "by [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
        "\n",
        "Gradient descent is the basic training method used in Deep Learning. The idea is very simple:\n",
        "\n",
        "- Initialize $W_0$ and $b_0$ randomly\n",
        "- For each step $t \\in [0 ; T-1]$:\n",
        "  - Sample a batch of examples $\\mathbf{y} = (y_i)_{1 \\leq i \\leq N}$\n",
        "  - Compute $\\nabla\\mathcal{L}_{W_t}(\\mathbf{y})$ and $\\nabla\\mathcal{L}_{b_t}(\\mathbf{y})$\n",
        "  - Update $W_{t+1} := W_t - \\alpha * \\nabla\\mathcal{L}_{W_t}(\\mathbf{y})$ and $b_{t+1} := b_t - \\alpha * \\nabla\\mathcal{L}_{b_t}(\\mathbf{y})$\n",
        "\n",
        "Here, $T$ is the number of iterations and $\\alpha$ is the **learning rate**. This algorithm is called a **training loop**."
      ],
      "metadata": {
        "id": "G5K48_qktN7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating the dataset { form-width: \"30%\" }\n",
        "\n",
        "# Create random dataset (Note that the seed make it deterministic)\n",
        "X = jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 6))\n",
        "Y = 12 * jnp.concatenate([X, X], axis=-1) + 6 + jax.random.normal(key=jax.random.PRNGKey(0), shape=(128, 12))"
      ],
      "metadata": {
        "id": "6hMKUMzrt1Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First implement the prediction function, which, given inputs $X$, weights $W$ and biases $b$ produces the output of the linear model $XW + b$."
      ],
      "metadata": {
        "id": "sCg3zOTnvcz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Linear prediction { form-width: \"30%\" }\n"
      ],
      "metadata": {
        "id": "7ce2N5kUuGBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Solution]** Linear prediction { form-width: \"30%\" }\n",
        "def predict(W: chex.Array, b: chex.Array, X: chex.Array) -> chex.Array:\n",
        "  return X @ W + b[None]"
      ],
      "metadata": {
        "id": "Fe1zwI0cxEGY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next implement the loss function that takes in the weights, biases, inputs and outputs, and produces the mean squared error."
      ],
      "metadata": {
        "id": "1jhobsNuxKhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Linear prediction { form-width: \"30%\" }\n"
      ],
      "metadata": {
        "id": "cZkBS-Y1xWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Solution]** Linear prediction { form-width: \"30%\" }\n",
        "def loss_fn(W: chex.Array, b: chex.Array, X: chex.Array, Y: chex.Array) -> chex.Array:\n",
        "  return jnp.mean(jnp.sum(jnp.square(predict(W, b, X) - Y), axis=-1))"
      ],
      "metadata": {
        "id": "Exe5kIGMxlll",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement an update function, that takes in the current parameters, all inputs and outputs, and a learning rate, and produces the parameters, once updated by performing one step of gradient descent. This function should also return the loss incurred with the current parameters."
      ],
      "metadata": {
        "id": "JZUK3ANSxt6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** Update function { form-width: \"30%\" }\n",
        "def update_fn(W: chex.Array, b: chex.Array, X: chex.Array, Y: chex.Array, learning_rate: float) -> Tuple[chex.Array, chex.Array, chex.Array]:\n",
        "  #### IMPLEMENT ####\n",
        "  return loss, W, b"
      ],
      "metadata": {
        "id": "wpIe7WphxrhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Solution]** Update function { form-width: \"30%\" }\n",
        "def update_fn(W: chex.Array, b: chex.Array, X: chex.Array, Y: chex.Array, learning_rate: float) -> Tuple[chex.Array, chex.Array, chex.Array]:\n",
        "  loss, (dW, db) = jax.value_and_grad(loss_fn, argnums=(0, 1))(W, b, X, Y)\n",
        "  new_W = W - learning_rate * dW\n",
        "  new_b = b - learning_rate * db\n",
        "  return loss, new_W, new_b"
      ],
      "metadata": {
        "id": "fO0RrdSXzmkC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JIT your function, so that it runs faster."
      ],
      "metadata": {
        "id": "cV-1vC-Zy3gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** JIT { form-width: \"30%\" }\n",
        "jitted_update_fn = ..."
      ],
      "metadata": {
        "id": "RQVdbizhy2w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Solution]** JIT { form-width: \"30%\" }\n",
        "jitted_update_fn = jax.jit(update_fn, static_argnums=4)"
      ],
      "metadata": {
        "id": "P_SAX-VTzu7y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize $W$ and $b$ either randomly or to constants (in the linear regression case, even constant initialization will do, since our loss function is convex)."
      ],
      "metadata": {
        "id": "MsUtKdEs0RR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize { form-width: \"30%\" }\n",
        "W = jnp.zeros((6, 12))\n",
        "b = jnp.zeros((12,))"
      ],
      "metadata": {
        "id": "7PR2MZLy0m8A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally check that your implementation is correct by running the following training loop. (The default hyperparameters will work, you can play with different hyperparameters to check the difference.) You should end up with $W$ being almost diagonal with only $12$ on the diagonal, and $b$ being constant with all entries equal to $6$."
      ],
      "metadata": {
        "id": "P0B_Ahjpz5cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Run]** Training loop { form-width: \"30%\" }\n",
        "num_iterations = 10_000 # @param\n",
        "learning_rate = .001 # @param\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  loss, W, b = jitted_update_fn(W, b, X, Y, learning_rate)\n",
        "  if i % 100 == 0:\n",
        "    print(f'At step {i},\\t loss: {loss}')"
      ],
      "metadata": {
        "id": "enLgfDLYz3NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you might have noticed, handling parameters in plain JAX is painful, as you need to keep track of absolutely all parameters (for a big network, this is going to become unsustainable). For this reason, we will learn to use Haiku, that exactly tackled this issue, in one of the next practicals."
      ],
      "metadata": {
        "id": "xFHJ5RSe2kWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. **[Bonus]** A simple Multi Layer Perceptron in JAX"
      ],
      "metadata": {
        "id": "j8wXie5RHxeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get more practice with JAX, you can now try to replicate the previous exercise, but this time using a 2 layer MLP with a ReLU activation in the middle instead of a linear function approximation. Remember that the output of a two layer MLP writes\n",
        "$$y = \\mathrm{relu}(x W_1 + b_1) W_2 + b_2\\cdot$$"
      ],
      "metadata": {
        "id": "jeIgxmsyIAJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating the dataset { form-width: \"30%\" }\n",
        "\n",
        "# Create random dataset (Note that the seed make it deterministic)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, *rngs = jax.random.split(rng, 5)\n",
        "# Train set\n",
        "X = jax.random.normal(key=rngs[0], shape=(128, 6))\n",
        "Y = 12 * jnp.concatenate([X, X], axis=-1) + 6 + jax.random.normal(key=rngs[1], shape=(128, 12))\n",
        "\n",
        "# Eval set\n",
        "X_eval = jax.random.normal(key=rngs[2], shape=(128, 6))\n",
        "Y_eval = 12 * jnp.concatenate([X_eval, X_eval], axis=-1) + 6 + jax.random.normal(key=rngs[3], shape=(128, 12))"
      ],
      "metadata": {
        "id": "J7pAtHCCH_Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Implement]** MLP regression{ form-width: \"30%\" }\n",
        "# Implement all the steps that you implemented for the linear regression for the MLP model.\n",
        "# Try to print the loss both on the train and eval sets, and see what happens for both losses\n",
        "num_hiddens = 32 # @param"
      ],
      "metadata": {
        "id": "lnDw4inKBcYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **[Solution]** MLP regression { form-width: \"30%\" }\n",
        "Params = Mapping[str, chex.Array]\n",
        "\n",
        "def predict(params: Params, X: chex.Array) -> chex.Array:\n",
        "  h = jax.nn.relu(X @ params['W1'] + params['b1'][None])\n",
        "  return h @ params['W2'] + params['b2'][None]\n",
        "\n",
        "def loss_fn(params: Params, X: chex.Array, Y: chex.Array) -> chex.Array:\n",
        "  return jnp.mean(jnp.sum(jnp.square(predict(params, X) - Y), axis=-1))\n",
        "\n",
        "def update_fn(params: Params, X: chex.Array, Y: chex.Array, learning_rate: float) -> Tuple[chex.Array, Params]:\n",
        "  loss, dparams = jax.value_and_grad(loss_fn, argnums=0)(params, X, Y)\n",
        "  new_params = jax.tree_util.tree_map(lambda x, dx: x - learning_rate * dx, params, dparams)\n",
        "  return loss, new_params\n",
        "\n",
        "def initialize(rng: chex.PRNGKey) -> Params:\n",
        "  rng1, rng2 = jax.random.split(rng)\n",
        "  W1 = jax.random.normal(rng1, (6, num_hiddens)) / jnp.sqrt(6)\n",
        "  W2 = jax.random.normal(rng2, (num_hiddens, 12)) / jnp.sqrt(num_hiddens)\n",
        "  b1 = jnp.zeros((num_hiddens,))\n",
        "  b2 = jnp.zeros((12,))\n",
        "  return dict(W1=W1, W2=W2, b1=b1, b2=b2)\n",
        "\n",
        "#@title **[Solution]** JIT { form-width: \"30%\" }\n",
        "jitted_update_fn = jax.jit(update_fn, static_argnums=3)\n",
        "jitted_loss = jax.jit(loss_fn)\n",
        "\n",
        "params = initialize(jax.random.PRNGKey(0))\n",
        "num_iterations = 100_000 # @param\n",
        "learning_rate = .001 # @param\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  loss, params = jitted_update_fn(params, X, Y, learning_rate)\n",
        "  eval_loss = jitted_loss(params, X_eval, Y_eval)\n",
        "  if i % 100 == 0:\n",
        "    print(f'At step {i},\\t train loss: {loss}\\t eval loss: {eval_loss}')"
      ],
      "metadata": {
        "id": "0LPRIsEGDk5M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pn-Y8I_oGAPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

